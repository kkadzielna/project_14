{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from collections import Counter\n",
    "from itertools import chain, combinations\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we compare the performance of multiple models on different subsets of our data.\n",
    "#### The models are:\n",
    "+ Logistic Regression\n",
    "+ SVM\n",
    "+ KNN\n",
    "+ Neural Network\n",
    "+ XGBoost\n",
    "\n",
    "#### The datasets:\n",
    "+ Mean/Mode imputed\n",
    "+ KNN imputed\n",
    "+ MICE imputed\n",
    "+ no imputations\n",
    "\n",
    "#### The targets:\n",
    "+ has_dep_diag\n",
    "+ a binary combination of all the target variables\n",
    "\n",
    "#### In combinations of:\n",
    "+ trained on balanced, tested on balanced\n",
    "+ trained on balanced, tested on imbalanced\n",
    "+ trained on imbalanced, tested on balanced\n",
    "+ trained on imbalanced, tested on imbalanced\n",
    "\n",
    "#### We employ  range of visualisation methods:\n",
    "+ ROC curves\n",
    "+ bar plots\n",
    "+ learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iputed datasets:\n",
    "+ 0 - unedited\n",
    "+ 1 - Mean/Mode\n",
    "+ 2/3? - drop NaN\n",
    "+ 4 - K-Means\n",
    "+ 5 - MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = pd.read_csv(\"newdata3.csv\", engine='python')\n",
    "data1 = pd.read_csv(\"imputed_dataset_1.csv\", engine='python').drop(['Unnamed: 0'], axis = 1)\n",
    "data2 = pd.read_csv(\"imputed_dataset_2.csv\", engine='python').drop(['Unnamed: 0'], axis = 1)\n",
    "data3 = pd.read_csv(\"imputed_dataset_3.csv\", engine='python').drop(['Unnamed: 0'], axis = 1)\n",
    "data4 = pd.read_csv(\"imputed_dataset_4.csv\", engine='python').drop(['Unnamed: 0'], axis = 1)\n",
    "data5 = pd.read_csv(\"imputed_dataset_5.csv\", engine='python').drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset 5 - combined variable (dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"imputed_dataset_5.csv\", engine='python').drop(['Unnamed: 0'], axis = 1)\n",
    "dep_data = data.copy()\n",
    "\n",
    "dep_num = np.array([12, 11, 10, 4])\n",
    "no_dep_num = np.setdiff1d(range(13), dep_num)\n",
    "\n",
    "dep_data[['prim_diag', 'secd_diag']] = dep_data[['prim_diag', 'secd_diag']].replace(list(no_dep_num), 0)\n",
    "dep_data[['prim_diag', 'secd_diag']] = dep_data[['prim_diag', 'secd_diag']].replace(list(dep_num), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = data.dropna(axis = 0, how = 'any')\n",
    "dep = [x for x in data.columns if 'dep' in x or 'diag' in x or 'panic' in x]\n",
    "dep_data['dep'] = dep_data['secd_diag'] + dep_data['prim_diag'] + 0 * dep_data['has_dep_diag']\n",
    "dep_data['dep'] = dep_data['dep'].replace(range(2, 4), 1)\n",
    "full = dep_data.dropna(axis = 0, how = 'any')\n",
    "dep = [x for x in data.columns if 'dep' in x or 'diag' in x or 'panic' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape Counter({0.0: 7723, 1.0: 277})\n"
     ]
    }
   ],
   "source": [
    "has_dep = full.query('dep == 1')\n",
    "no_dep = full.query('dep == 0')\n",
    "size = int(np.round(0.8 * min(len(has_dep), len(no_dep))))\n",
    "#sample = pd.concat([has_dep.sample(size), no_dep.sample(size)])\n",
    "sample = full.sample(8000)\n",
    "\n",
    "sample = sample.sort_index()\n",
    "sample = sample.reset_index(drop = True)\n",
    "\n",
    "\n",
    "X_comb = sample.drop(dep, axis = 1).drop('dep', axis = 1)\n",
    "Y_comb = sample['dep']\n",
    "print('dataset shape %s' % Counter(Y_comb))\n",
    "\n",
    "#sample = full.sample(8000)\n",
    "#smote = SMOTE(random_state = 0)\n",
    "#X, y = smote.fit_resample(sample.drop(dep, axis = 1).drop('dep', axis = 1), sample['dep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset 5 - has_dep_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13734, 36)\n",
      "(13734,)\n",
      "dataset shape Counter({0.0: 13344, 1.0: 390})\n"
     ]
    }
   ],
   "source": [
    "X_hasdep = data5.drop(['has_dep_diag'],axis=1).drop(['secd_diag'],axis=1).drop(['prim_diag'],axis=1).drop(['dep_score'],axis=1).drop(['dep_thoughts'],axis=1).drop(['panic_score'], axis=1)\n",
    "print(X_hasdep.shape)\n",
    "Y_hasdep = np.array(data5['has_dep_diag'])\n",
    "print(Y_hasdep.shape)\n",
    "print('dataset shape %s' % Counter(Y_hasdep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over/undersampling to obtain imbalanced and balanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 35)\n",
      "(2000,)\n",
      "Resampled dataset shape Counter({1.0: 1880, 0.0: 120})\n",
      "(2000, 35)\n",
      "(2000,)\n",
      "Resampled dataset shape Counter({0.0: 1000, 1.0: 1000})\n",
      "(2000, 36)\n",
      "(2000,)\n",
      "Resampled dataset shape Counter({1.0: 1880, 0.0: 120})\n",
      "(2000, 36)\n",
      "(2000,)\n",
      "Resampled dataset shape Counter({0.0: 1000, 1.0: 1000})\n"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN \n",
    "from imblearn import under_sampling\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "\n",
    "nb = 1000\n",
    "nl = 1880\n",
    "ns = 120\n",
    "\n",
    "rusboost = RUSBoostClassifier(sampling_strategy={1.0: 400, 0.0: 400}, random_state=42)\n",
    "\n",
    "#combined, imbalanced (original ratio), undersampling\n",
    "smoteenn = SMOTEENN(random_state = 42, sampling_strategy=1.0)\n",
    "rus = under_sampling.RandomUnderSampler(sampling_strategy={1.0: nl, 0.0: ns}, random_state=42)\n",
    "X_combined_imb, Y_combined_imb = smoteenn.fit_resample(X_comb, Y_comb)\n",
    "X_combined_imb, Y_combined_imb = rus.fit_resample(X_combined_imb, Y_combined_imb)\n",
    "print(X_combined_imb.shape)\n",
    "print(Y_combined_imb.shape)\n",
    "print('Resampled dataset shape %s' % Counter(Y_combined_imb))\n",
    "\n",
    "#combined, imbalanced (original ratio), oversampling\n",
    "\n",
    "#combined, balanced, undersampling\n",
    "smoteenn = SMOTEENN(random_state = 42, sampling_strategy=1.0)\n",
    "rus = under_sampling.RandomUnderSampler(sampling_strategy={1.0: nb, 0.0: nb}, random_state=42)\n",
    "X_combined_b, Y_combined_b = smoteenn.fit_resample(X_comb, Y_comb)\n",
    "X_combined_b, Y_combined_b = rus.fit_resample(X_combined_b, Y_combined_b)\n",
    "#rusboost.fit(X_combined_b, Y_combined_b)\n",
    "#Y_combined_b = rusboost.predict(X_combined_b)\n",
    "print(X_combined_b.shape)\n",
    "print(Y_combined_b.shape)\n",
    "print('Resampled dataset shape %s' % Counter(Y_combined_b))\n",
    "\n",
    "#combined, balanced, oversampling\n",
    "\n",
    "#has_dep_diag, imbalanced (original ratio), undersampling\n",
    "smoteenn = SMOTEENN(random_state = 42, sampling_strategy=1.0)\n",
    "rus = under_sampling.RandomUnderSampler(sampling_strategy={1.0: nl, 0.0: ns}, random_state=42)\n",
    "X_hasdep_imb, Y_hasdep_imb = smoteenn.fit_resample(X_hasdep, Y_hasdep)\n",
    "X_hasdep_imb, Y_hasdep_imb = rus.fit_resample(X_hasdep_imb, Y_hasdep_imb)\n",
    "print(X_hasdep_imb.shape)\n",
    "print(Y_hasdep_imb.shape)\n",
    "print('Resampled dataset shape %s' % Counter(Y_hasdep_imb))\n",
    "\n",
    "#has_dep_diag, imbalanced (original ratio), oversampling\n",
    "\n",
    "#has_dep_diag, balanced, undersampling\n",
    "smoteenn = SMOTEENN(random_state = 42, sampling_strategy=1.0)\n",
    "rus = under_sampling.RandomUnderSampler(sampling_strategy={1.0: nb, 0.0: nb}, random_state=42)\n",
    "X_hasdep_b, Y_hasdep_b = smoteenn.fit_resample(X_hasdep, Y_hasdep)\n",
    "X_hasdep_b, Y_hasdep_b = rus.fit_resample(X_hasdep_b, Y_hasdep_b)\n",
    "print(X_hasdep_b.shape)\n",
    "print(Y_hasdep_b.shape)\n",
    "print('Resampled dataset shape %s' % Counter(Y_hasdep_b))\n",
    "\n",
    "#has_dep_diag, balanced, oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def shuffle_dataset(N, X, y, X_shuffled, y_shuffled):\n",
    "    ind_list = [i for i in range(N)]\n",
    "    shuffle(ind_list)\n",
    "    X_shuffled  = X.iloc[ind_list]\n",
    "    y_shuffled = y.iloc[ind_list]\n",
    "    \n",
    "def split_dataset(split, N, X, y):\n",
    "    X_shuffled = X\n",
    "    y_shuffled = y\n",
    "    shuffle_dataset(N, pd. DataFrame(X), pd. DataFrame(y), pd. DataFrame(X_shuffled), pd. DataFrame(y_shuffled))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled,test_size=split, random_state=40)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 35)\n",
      "(400, 35)\n",
      "(1600,)\n",
      "(400,)\n",
      "(1600, 35)\n",
      "(400, 35)\n",
      "(1600,)\n",
      "(400,)\n",
      "(1600, 36)\n",
      "(400, 36)\n",
      "(1600,)\n",
      "(400,)\n",
      "(1600, 36)\n",
      "(400, 36)\n",
      "(1600,)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "#combined, imbalanced (original ratio), undersampling\n",
    "X_train_comb_imb, X_test_comb_imb, y_train_comb_imb, y_test_comb_imb = split_dataset(0.2, Y_combined_imb.size, X_combined_imb, Y_combined_imb)\n",
    "print(X_train_comb_imb.shape); print(X_test_comb_imb.shape)\n",
    "print(y_train_comb_imb.shape); print(y_test_comb_imb.shape)\n",
    "\n",
    "#combined, balanced, undersampling\n",
    "X_train_comb_b, X_test_comb_b, y_train_comb_b, y_test_comb_b = split_dataset(0.2, Y_combined_b.size, X_combined_b, Y_combined_b)\n",
    "print(X_train_comb_b.shape); print(X_test_comb_b.shape)\n",
    "print(y_train_comb_b.shape); print(y_test_comb_b.shape)\n",
    "\n",
    "#has_dep_diag, imbalanced (original ratio), undersampling\n",
    "X_train_hasdep_imb, X_test_hasdep_imb, y_train_hasdep_imb, y_test_hasdep_imb = split_dataset(0.2, Y_hasdep_imb.size, X_hasdep_imb, Y_hasdep_imb)\n",
    "print(X_train_hasdep_imb.shape); print(X_test_hasdep_imb.shape)\n",
    "print(y_train_hasdep_imb.shape); print(y_test_hasdep_imb.shape)\n",
    "\n",
    "#has_dep_diag, balanced, undersampling\n",
    "X_train_hasdep_b, X_test_hasdep_b, y_train_hasdep_b, y_test_hasdep_b = split_dataset(0.2, Y_hasdep_b.size, X_hasdep_b, Y_hasdep_b)\n",
    "print(X_train_hasdep_b.shape); print(X_test_hasdep_b.shape)\n",
    "print(y_train_hasdep_b.shape); print(y_test_hasdep_b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4865211810012837\n",
      "0.3464052287581699\n",
      "0.0498812351543943\n",
      "0.31972789115646255\n",
      "0.4865211810012837\n",
      "0.3464052287581699\n",
      "0.0498812351543943\n",
      "0.31972789115646255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty = 'l2', max_iter = 1000, C = 1e-07, solver = 'newton-cg')\n",
    "\n",
    "#combined variable\n",
    "\n",
    "#train imbalanced, test imbalanced\n",
    "clf.fit(X_train_comb_imb, y_train_comb_imb)\n",
    "y_pred = clf.predict(X_test_comb_imb)\n",
    "print(f1_score(y_pred, y_test_comb_imb, average = 'macro'))\n",
    "\n",
    "#train imbalanced, test balanced\n",
    "clf.fit(X_train_comb_imb, y_train_comb_imb)\n",
    "y_pred = clf.predict(X_test_comb_b)\n",
    "print(f1_score(y_pred, y_test_comb_b, average = 'macro'))\n",
    "\n",
    "#train balanced, test imbalanced\n",
    "clf.fit(X_train_comb_b, y_train_comb_b)\n",
    "y_pred = clf.predict(X_test_comb_imb)\n",
    "print(f1_score(y_pred, y_test_comb_imb, average = 'macro'))\n",
    "\n",
    "#train balanced, test balanced\n",
    "clf.fit(X_train_comb_b, y_train_comb_b)\n",
    "y_pred = clf.predict(X_test_comb_b)\n",
    "print(f1_score(y_pred, y_test_comb_b, average = 'macro'))\n",
    "\n",
    "\n",
    "#has_dep_diag\n",
    "\n",
    "#train imbalanced, test imbalanced\n",
    "clf.fit(X_train_hasdep_imb, y_train_hasdep_imb)\n",
    "y_pred = clf.predict(X_test_hasdep_imb)\n",
    "print(f1_score(y_pred, y_test_hasdep_imb, average = 'macro'))\n",
    "\n",
    "#train imbalanced, test balanced\n",
    "clf.fit(X_train_hasdep_imb, y_train_hasdep_imb)\n",
    "y_pred = clf.predict(X_test_hasdep_b)\n",
    "print(f1_score(y_pred, y_test_hasdep_b, average = 'macro'))\n",
    "\n",
    "#train balanced, test imbalanced\n",
    "clf.fit(X_train_hasdep_b, y_train_hasdep_b)\n",
    "y_pred = clf.predict(X_test_hasdep_imb)\n",
    "print(f1_score(y_pred, y_test_hasdep_imb, average = 'macro'))\n",
    "\n",
    "#train balanced, test balanced\n",
    "clf.fit(X_train_hasdep_b, y_train_hasdep_b)\n",
    "y_pred = clf.predict(X_test_hasdep_b)\n",
    "print(f1_score(y_pred, y_test_hasdep_b, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912739965095986\n",
      "0.8262014483212639\n",
      "0.9008182494421026\n",
      "0.8815775924209659\n",
      "0.9447437491366211\n",
      "0.8031862549535893\n",
      "0.9113475177304965\n",
      "0.8876375801220664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5) #, metric='euclidean')\n",
    "\n",
    "#combined variable\n",
    "\n",
    "#train imbalanced, test imbalanced\n",
    "knn.fit(X_train_comb_imb, y_train_comb_imb)\n",
    "y_pred = knn.predict(X_test_comb_imb)\n",
    "print(f1_score(y_pred, y_test_comb_imb, average = 'macro'))\n",
    "\n",
    "#train imbalanced, test balanced\n",
    "knn.fit(X_train_comb_imb, y_train_comb_imb)\n",
    "y_pred = knn.predict(X_test_comb_b)\n",
    "print(f1_score(y_pred, y_test_comb_b, average = 'macro'))\n",
    "\n",
    "#train balanced, test imbalanced\n",
    "knn.fit(X_train_comb_b, y_train_comb_b)\n",
    "y_pred = knn.predict(X_test_comb_imb)\n",
    "print(f1_score(y_pred, y_test_comb_imb, average = 'macro'))\n",
    "\n",
    "#train balanced, test balanced\n",
    "knn.fit(X_train_comb_b, y_train_comb_b)\n",
    "y_pred = knn.predict(X_test_comb_b)\n",
    "print(f1_score(y_pred, y_test_comb_b, average = 'macro'))\n",
    "\n",
    "\n",
    "#has_dep_diag\n",
    "\n",
    "#train imbalanced, test imbalanced\n",
    "knn.fit(X_train_hasdep_imb, y_train_hasdep_imb)\n",
    "y_pred = knn.predict(X_test_hasdep_imb)\n",
    "print(f1_score(y_pred, y_test_hasdep_imb, average = 'macro'))\n",
    "\n",
    "#train imbalanced, test balanced\n",
    "knn.fit(X_train_hasdep_imb, y_train_hasdep_imb)\n",
    "y_pred = knn.predict(X_test_hasdep_b)\n",
    "print(f1_score(y_pred, y_test_hasdep_b, average = 'macro'))\n",
    "\n",
    "#train balanced, test imbalanced\n",
    "knn.fit(X_train_hasdep_b, y_train_hasdep_b)\n",
    "y_pred = knn.predict(X_test_hasdep_imb)\n",
    "print(f1_score(y_pred, y_test_hasdep_imb, average = 'macro'))\n",
    "\n",
    "#train balanced, test balanced\n",
    "knn.fit(X_train_hasdep_b, y_train_hasdep_b)\n",
    "y_pred = knn.predict(X_test_hasdep_b)\n",
    "print(f1_score(y_pred, y_test_hasdep_b, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912739965095986\n",
      "0.8089983022071308\n",
      "0.9357305826022686\n",
      "0.9289772727272727\n",
      "0.9447437491366211\n",
      "0.8060973099322004\n",
      "0.9335106382978724\n",
      "0.8960650480699152\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(C=5, gamma='auto', kernel='rbf')\n",
    "\n",
    "#combined variable\n",
    "\n",
    "#train imbalanced, test imbalanced\n",
    "svm.fit(X_train_comb_imb, y_train_comb_imb)\n",
    "y_pred = svm.predict(X_test_comb_imb)\n",
    "print(f1_score(y_pred, y_test_comb_imb, average = 'macro'))\n",
    "\n",
    "#train imbalanced, test balanced\n",
    "svm.fit(X_train_comb_imb, y_train_comb_imb)\n",
    "y_pred = svm.predict(X_test_comb_b)\n",
    "print(f1_score(y_pred, y_test_comb_b, average = 'macro'))\n",
    "\n",
    "#train balanced, test imbalanced\n",
    "svm.fit(X_train_comb_b, y_train_comb_b)\n",
    "y_pred = svm.predict(X_test_comb_imb)\n",
    "print(f1_score(y_pred, y_test_comb_imb, average = 'macro'))\n",
    "\n",
    "#train balanced, test balanced\n",
    "svm.fit(X_train_comb_b, y_train_comb_b)\n",
    "y_pred = svm.predict(X_test_comb_b)\n",
    "print(f1_score(y_pred, y_test_comb_b, average = 'macro'))\n",
    "\n",
    "\n",
    "#has_dep_diag\n",
    "\n",
    "#train imbalanced, test imbalanced\n",
    "svm.fit(X_train_hasdep_imb, y_train_hasdep_imb)\n",
    "y_pred = svm.predict(X_test_hasdep_imb)\n",
    "print(f1_score(y_pred, y_test_hasdep_imb, average = 'macro'))\n",
    "\n",
    "#train imbalanced, test balanced\n",
    "svm.fit(X_train_hasdep_imb, y_train_hasdep_imb)\n",
    "y_pred = svm.predict(X_test_hasdep_b)\n",
    "print(f1_score(y_pred, y_test_hasdep_b, average = 'macro'))\n",
    "\n",
    "#train balanced, test imbalanced\n",
    "svm.fit(X_train_hasdep_b, y_train_hasdep_b)\n",
    "y_pred = svm.predict(X_test_hasdep_imb)\n",
    "print(f1_score(y_pred, y_test_hasdep_imb, average = 'macro'))\n",
    "\n",
    "#train balanced, test balanced\n",
    "svm.fit(X_train_hasdep_b, y_train_hasdep_b)\n",
    "y_pred = svm.predict(X_test_hasdep_b)\n",
    "print(f1_score(y_pred, y_test_hasdep_b, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 36), found shape=(32, 35)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b8c42d4c86ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mneurons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m36\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNN_Model_Dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfinal_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_comb_imb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_comb_imb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test_comb_imb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_comb_imb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_comb_imb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_comb_imb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/karolina/anaconda3/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 36), found shape=(32, 35)\n"
     ]
    }
   ],
   "source": [
    "from FFNeuralNetwork import NN_models\n",
    "\n",
    "neurons = [36, 36]\n",
    "fnn = NN_models.NN_Model_Dropout(neuron = neurons, activation = 'relu', in_shape = (36,), classes = 2, output_activation = 'sigmoid', optimizer = 'Adam', loss = 'sparse_categorical_crossentropy', dr = 0.4)\n",
    "final_history = fnn.fit(X_train_comb_imb, y_train_comb_imb, epochs = 100, validation_data = (X_test_comb_imb, y_test_comb_imb))\n",
    "score = fnn.evaluate(X_test_comb_imb, y_test_comb_imb, verbose = 0, batch_size = 32)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 : 0.9127\n",
      "f1 : 0.8148\n",
      "f1 : 0.9058\n",
      "f1 : 0.9699\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-66a188b27223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m#train imbalanced, test imbalanced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mmodelfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_hasdep_imb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_hasdep_imb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_hasdep_imb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_hasdep_imb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#train imbalanced, test balanced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-66a188b27223>\u001b[0m in \u001b[0;36mmodelfit\u001b[0;34m(alg, dtrain_X, dtrain_Y, dtest_X, dtest_Y, useTrainCV, cv_folds, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mxgb_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xgb_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mxgtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mxgtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mxgtest_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics \n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "#from xgboost import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "from pandas import MultiIndex, Int64Index\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def modelfit(alg, dtrain_X, dtrain_Y, dtest_X, dtest_Y, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "\n",
    "\n",
    "    xgb_param = alg.get_xgb_params()\n",
    "        \n",
    "    xgtrain = xgb.DMatrix(dtrain_X.values, dtrain_Y.values)\n",
    "    xgtest_X = xgb.DMatrix(dtest_X)\n",
    "    xgtest_Y = xgb.DMatrix(dtest_Y)\n",
    "        \n",
    "    cvresult = xgb.cv(xgb_param, \n",
    "                     xgtrain, \n",
    "                     num_boost_round=alg.get_params()['n_estimators'], \n",
    "                     nfold=cv_folds, \n",
    "                      metrics='auc',\n",
    "                    early_stopping_rounds=early_stopping_rounds)\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    alg.fit(dtrain_X, dtrain_Y,eval_metric=['logloss','auc','error'])\n",
    "\n",
    "    # pred\n",
    "    dtrain_predictions = alg.predict(dtest_X)\n",
    "    dtrain_predprob = alg.predict_proba(dtest_X)[:,1]\n",
    "\n",
    "    print (\"f1 : %.4g\" % metrics.f1_score(dtest_Y, dtrain_predictions, average = 'macro'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xgbc = XGBClassifier(n_estimators=1000,\n",
    "                         learning_rate=0.05, \n",
    "                         n_jobs=4,\n",
    "                         max_depth = 16,\n",
    "                         min_child_weight = 1,\n",
    "                         gamma = 0.2,\n",
    "                         subsample=0.97, \n",
    "                         colsample_bytree=0.73,\n",
    "                         reg_alpha = 3.64,\n",
    "                         seed=1024,\n",
    "                         use_label_encoder=False)\n",
    "\n",
    "\n",
    "#combined variable\n",
    "\n",
    "#train imbalanced, test imbalanced\n",
    "modelfit(xgbc, X_train_comb_imb, y_train_comb_imb, X_test_comb_imb, y_test_comb_imb)\n",
    "\n",
    "#train imbalanced, test balanced\n",
    "modelfit(xgbc, X_train_comb_imb, y_train_comb_imb, X_test_comb_b, y_test_comb_b)\n",
    "\n",
    "#train balanced, test imbalanced\n",
    "modelfit(xgbc, X_train_comb_b, y_train_comb_b, X_test_comb_imb, y_test_comb_imb)\n",
    "\n",
    "#train balanced, test balanced\n",
    "modelfit(xgbc, X_train_comb_b, y_train_comb_b, X_test_comb_b, y_test_comb_b)\n",
    "\n",
    "\n",
    "#has_dep_diag\n",
    "\n",
    "#train imbalanced, test imbalanced\n",
    "modelfit(xgbc, X_train_hasdep_imb, y_train_hasdep_imb, X_test_hasdep_imb, y_test_hasdep_imb)\n",
    "\n",
    "#train imbalanced, test balanced\n",
    "modelfit(xgbc, X_train_hasdep_imb, y_train_hasdep_imb, X_test_hasdep_b, y_test_hasdep_b)\n",
    "\n",
    "#train balanced, test imbalanced\n",
    "modelfit(xgbc, X_train_hasdep_b, y_train_hasdep_b, X_test_hasdep_imb, y_test_hasdep_imb)\n",
    "\n",
    "#train balanced, test balanced\n",
    "modelfit(xgbc, X_train_hasdep_b, y_train_hasdep_b, X_test_hasdep_b, y_test_hasdep_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
