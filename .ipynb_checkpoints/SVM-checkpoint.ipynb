{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from collections import Counter\n",
    "from itertools import chain, combinations\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = pd.read_csv(\"newdata3.csv\", engine='python')\n",
    "data1 = pd.read_csv(\"imputed_dataset_1.csv\", engine='python')\n",
    "data1.head()\n",
    "data2 = pd.read_csv(\"imputed_dataset_2.csv\", engine='python')\n",
    "data2.head()\n",
    "data3 = pd.read_csv(\"imputed_dataset_3.csv\", engine='python')\n",
    "data3.head()\n",
    "data4 = pd.read_csv(\"imputed_dataset_4.csv\", engine='python')\n",
    "data4.head()\n",
    "data5 = pd.read_csv(\"imputed_dataset_5.csv\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>weight_16</th>\n",
       "      <th>height_16</th>\n",
       "      <th>comp_noint_bed_16</th>\n",
       "      <th>comp_int_bed_16</th>\n",
       "      <th>talk_phon_wend</th>\n",
       "      <th>text_wend</th>\n",
       "      <th>talk_mob_wend</th>\n",
       "      <th>comp_wend</th>\n",
       "      <th>musi_wend</th>\n",
       "      <th>...</th>\n",
       "      <th>anx_band_15</th>\n",
       "      <th>exercise</th>\n",
       "      <th>child_bull</th>\n",
       "      <th>has_dep_diag</th>\n",
       "      <th>secd_diag</th>\n",
       "      <th>prim_diag</th>\n",
       "      <th>panic_score</th>\n",
       "      <th>dep_thoughts</th>\n",
       "      <th>dep_score</th>\n",
       "      <th>comp_house</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>59.294132</td>\n",
       "      <td>181.602831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>62.290242</td>\n",
       "      <td>168.453878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>62.290242</td>\n",
       "      <td>168.453878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>49.812426</td>\n",
       "      <td>160.224186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>62.270030</td>\n",
       "      <td>191.703227</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  weight_16   height_16  comp_noint_bed_16  comp_int_bed_16  \\\n",
       "0           0  59.294132  181.602831                0.0              0.0   \n",
       "1           1  62.290242  168.453878                0.0              0.0   \n",
       "2           2  62.290242  168.453878                0.0              0.0   \n",
       "3           3  49.812426  160.224186                0.0              0.0   \n",
       "4           4  62.270030  191.703227                1.0              0.0   \n",
       "\n",
       "   talk_phon_wend  text_wend  talk_mob_wend  comp_wend  musi_wend  ...  \\\n",
       "0             1.0        1.0            1.0        2.0        0.0  ...   \n",
       "1             1.0        1.0            1.0        2.0        0.0  ...   \n",
       "2             1.0        1.0            1.0        2.0        0.0  ...   \n",
       "3             1.0        2.0            1.0        2.0        0.0  ...   \n",
       "4             0.0        0.0            0.0        1.0        0.0  ...   \n",
       "\n",
       "   anx_band_15  exercise  child_bull  has_dep_diag  secd_diag  prim_diag  \\\n",
       "0          0.5       4.0         0.0           0.0        0.0        0.0   \n",
       "1          3.0       3.0         0.0           0.0        1.0        1.0   \n",
       "2          3.0       3.0         0.0           0.0        1.0        1.0   \n",
       "3          3.0       3.0         0.0           0.0        0.0        1.0   \n",
       "4          3.0       3.0         0.0           0.0        1.0        1.0   \n",
       "\n",
       "   panic_score  dep_thoughts  dep_score  comp_house  \n",
       "0          0.0           1.0        0.0         1.0  \n",
       "1          0.0           3.0        1.0         0.0  \n",
       "2          0.0           3.0        1.0         0.0  \n",
       "3          0.0           2.0        0.0         0.0  \n",
       "4          0.0           3.0        0.0         0.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train an SVM (with a specific Kernel), plot the training and validation learning curves. You may need to subsample the dataset if SVM training is taking too long. Do you see any signs of overfitting? Interpret and discuss your results. (1 mark)\n",
    "2. What are your results in the testing dataset? Interpret and discuss your results. (2 marks)\n",
    "3. How sensitive is this method to different hyperparameters? For example the different types of kernel (e.g. linear, RBF, etc.). Make use of plots (e.g. performance on test dataset as a function of different hyperparameters) to help you discuss this point. (5 marks)\n",
    "4. Plot decision boundaries and discuss their relevance. (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate target from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13734, 37)\n",
      "(13734, 5)\n"
     ]
    }
   ],
   "source": [
    "X = data5.drop(['has_dep_diag'],axis=1).drop(['secd_diag'],axis=1).drop(['prim_diag'],axis=1).drop(['dep_score'],axis=1).drop(['dep_thoughts'],axis=1).drop(['panic_score'], axis=1)\n",
    "print(X.shape)\n",
    "Y = np.column_stack((data5['has_dep_diag'], data5['secd_diag'], data5['prim_diag'], data5['dep_score'], data5['dep_thoughts']))\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly shuffle the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def shuffle_dataset(N, X, y, X_shuffled, y_shuffled):\n",
    "    ind_list = [i for i in range(N)]\n",
    "    shuffle(ind_list)\n",
    "    X_shuffled  = X.iloc[ind_list]\n",
    "    y_shuffled = y.iloc[ind_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuffled = X\n",
    "y_shuffled = Y\n",
    "shuffle_dataset(13734, pd. DataFrame(X), pd. DataFrame(Y), pd. DataFrame(X_shuffled), pd. DataFrame(y_shuffled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9613, 37)\n",
      "(4121, 37)\n",
      "(9613, 5)\n",
      "(4121, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled,test_size=0.3, random_state=40)\n",
    "print(X_train.shape); print(X_test.shape)\n",
    "print(y_train.shape); print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 37)\n",
      "(200, 37)\n",
      "(400, 5)\n",
      "(200, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train_reduced, y_train_reduced = X_train[0:400], y_train[0:400]\n",
    "X_test_reduced, y_test_reduced = X_test[0:200], y_test[0:200]\n",
    "print(X_train_reduced.shape); print(X_test_reduced.shape)\n",
    "print(y_train_reduced.shape); print(y_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 381, 1.0: 19})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_train_reduced[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model:\n",
    "Imputed dataset 5, for has_dep_diag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic SVM, with accuracy and f1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM classifier accuracy: 0.9679600540934151\n",
      "SVM classifier accuracy - test set: 0.9694248968696918\n",
      "0.5075785697844445\n",
      "0.500048142100074\n"
     ]
    }
   ],
   "source": [
    "##imbalanced\n",
    "svm = SVC(C=0.1, gamma='scale', kernel='linear')\n",
    "svm.fit(X_train, y_train[:,0])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,0])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,0])}')\n",
    "\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,0], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,0], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM classifier accuracy: 0.6780401539581816\n",
      "SVM classifier accuracy - test set: 0.6704683329289007\n",
      "0.45348099565527067\n",
      "0.4413915706425828\n"
     ]
    }
   ],
   "source": [
    "##auto balancing\n",
    "svm = SVC(C=0.1, gamma='scale', kernel='linear', class_weight = 'balanced')\n",
    "svm.fit(X_train, y_train[:,0])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,0])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,0])}')\n",
    "\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,0], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,0], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For diff hyperparameters: here the train set accuracy is lower than the test one as it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM classifier accuracy: 0.9849162592322896\n",
      "SVM classifier accuracy - test set: 0.9728221305508372\n",
      "0.8197476647055819\n",
      "0.49311193111931123\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(C=1, gamma = 'auto')\n",
    "svm.fit(X_train, y_train[:,0])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,0])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,0])}')\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,0], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,0], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the other targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM classifier accuracy: 0.7261000728180589\n",
      "SVM classifier accuracy - test set: 0.7260373695704926\n",
      "0.14368923833108113\n",
      "0.17886816235287073\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(C=0.1, gamma='scale', kernel='linear')\n",
    "svm.fit(X_train, y_train[:,1])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,1])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,1])}')\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,1], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,1], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=0.1, gamma='scale', kernel='linear')\n",
    "svm.fit(X_train, y_train[:,2])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,2])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,2])}')\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,2], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,2], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=0.1, gamma='scale', kernel='linear')\n",
    "svm.fit(X_train, y_train[:,3])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,3])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,3])}')\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,3], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,3], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=0.1, gamma='scale', kernel='linear')\n",
    "svm.fit(X_train_reduced, y_train_reduced[:,4])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,4])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,4])}')\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,4], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,4], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "#plot learning curve function from the labs\n",
    "def plot_learning_curve(n, est, xs, ys, title):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(estimator = est, X = xs, y = ys, cv = n)\n",
    "\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    valid_mean = np.mean(valid_scores, axis=1)\n",
    "\n",
    "    plt.plot(train_sizes, train_mean, color='blue', label='Training Accuracy')\n",
    "    plt.plot(train_sizes, valid_mean, color='green', label='Validation Accuracy')\n",
    "\n",
    "    plt.xlabel('Dataset size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(5, svm,X_train_reduced, y_train_reduced[:,0], 'SVM learning curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "def plot_validation_curve(est, X, y, p_name, p_range, title):\n",
    "    train_scores, test_scores = validation_curve(estimator=est,\n",
    "                                             X=X, y=y,\n",
    "                                             cv=5,\n",
    "    param_name=p_name, param_range=p_range)\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.plot(p_range, train_mean,\n",
    "             color='blue', label='Training Accuracy')\n",
    "    plt.plot(p_range, test_mean,\n",
    "             color='green', label='Validation Accuracy')\n",
    "    plt.xlabel(p_name)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_curve(svm, X_train_reduced, y_train_reduced[:,0], 'C', list(np.arange(0.0, 10, 0.1))\n",
    ", \"SVM validation curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hypermarameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##on accuracy\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.0, 0.1,1, 5, 10, 100], \n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid']\n",
    "}\n",
    "tuned_svm = GridSearchCV(svm, param_grid, cv = 5, scoring='accuracy', verbose = 2, n_jobs=-1)\n",
    "tuned_svm.fit(X_train_reduced, y_train_reduced[:,0])\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "params = tuned_svm.best_params_\n",
    "\n",
    "print(params)\n",
    "print(tuned_svm.score(X_train,y_train[:,0]))\n",
    "print(tuned_svm.score(X_test,y_test[:,0]))\n",
    "y_pred = tuned_svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,0], average = \"macro\"))\n",
    "y_pred_test = tuned_svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,0], average = \"macro\"))\n",
    "print(str((end - start)/3600) + mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on f1 macro\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.0, 0.1,1, 5, 10, 100], \n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid']\n",
    "}\n",
    "tuned_svm = GridSearchCV(svm, param_grid, cv = 5, scoring='f1_macro' , verbose = 2, n_jobs=-1)\n",
    "tuned_svm.fit(X_train_reduced, y_train_reduced[:,0])\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "params = tuned_svm.best_params_\n",
    "\n",
    "print(params)\n",
    "print(tuned_svm.score(X_train,y_train[:,0]))\n",
    "print(tuned_svm.score(X_test,y_test[:,0]))\n",
    "y_pred = tuned_svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,0], average = \"macro\"))\n",
    "y_pred_test = tuned_svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,0], average = \"macro\"))\n",
    "\n",
    "\n",
    "print(str((end - start)/3600) + mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "## the largst nu I was allowed\n",
    "svm = NuSVC(nu = 0.05)\n",
    "svm.fit(X_train, y_train[:,0])\n",
    "\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,0])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,0])}')\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,0], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,0], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On normalized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "normalized_data = preprocessing.normalize(X_shuffled)\n",
    "normalized_target = preprocessing.normalize(y_shuffled)\n",
    "print(normalized_data[0,:])\n",
    "print(normalized_target[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(normalized_data, y_shuffled,test_size=0.3, random_state=40)\n",
    "print(X_train_norm.shape); print(X_test_norm.shape)\n",
    "print(y_train_norm.shape); print(y_test_norm.shape)\n",
    "X_train_reduced_norm, y_train_reduced_norm = X_train_norm[0:400], y_train_norm[0:400]\n",
    "X_test_reduced_norm, y_test_reduced_norm = X_test_norm[0:200], y_test_norm[0:200]\n",
    "print(X_train_reduced_norm.shape); print(X_test_reduced_norm.shape)\n",
    "print(y_train_reduced_norm.shape); print(y_test_reduced_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imbalanced\n",
    "svm_norm = SVC(C=0.1, gamma='scale', kernel='linear')\n",
    "svm_norm.fit(X_train_norm, y_train_norm[:,0])\n",
    "print(f'SVM classifier accuracy: {svm_norm.score(X_train_norm, y_train_norm[:,0])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm_norm.score(X_test_norm, y_test_norm[:,0])}')\n",
    "\n",
    "y_pred_norm = svm_norm.predict(X_train_norm)\n",
    "print(f1_score(y_pred_norm, y_train_norm[:,0], average = \"macro\"))\n",
    "y_pred_test_norm = svm_norm.predict(X_test_norm)\n",
    "print(f1_score(y_pred_test_norm, y_test_norm[:,0], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##auto balancing\n",
    "##has_dep_diag\n",
    "svm_norm = SVC(C=0.1, gamma='scale', kernel='linear', class_weight = 'balanced')\n",
    "svm_norm.fit(X_train_norm, y_train_norm[:,0])\n",
    "print(f'SVM classifier accuracy: {svm_norm.score(X_train_norm, y_train_norm[:,0])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm_norm.score(X_test_norm, y_test_norm[:,0])}')\n",
    "\n",
    "y_pred_norm = svm_norm.predict(X_train_norm)\n",
    "print(f1_score(y_pred_norm, y_train_norm[:,0], average = \"macro\"))\n",
    "y_pred_test_norm = svm_norm.predict(X_test_norm)\n",
    "print(f1_score(y_pred_test_norm, y_test_norm[:,0], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##auto balancing\n",
    "##secd_diag\n",
    "svm = SVC(C=0.1, gamma='scale', kernel='linear', class_weight = 'balanced')\n",
    "svm.fit(X_train, y_train[:,1])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,1])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,1])}')\n",
    "\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,1], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,1], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##auto balancing\n",
    "##prim_diag\n",
    "svm = SVC(C=0.1, gamma='scale', kernel='linear', class_weight = 'balanced')\n",
    "svm.fit(X_train, y_train[:,2])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,2])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,2])}')\n",
    "\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,2], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,2], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##auto balancing\n",
    "##dep_score\n",
    "svm = SVC(C=0.1, gamma='scale', kernel='linear', class_weight = 'balanced')\n",
    "svm.fit(X_train, y_train[:,3])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,3])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,3])}')\n",
    "\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,3], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,3], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##auto balancing\n",
    "##dep_thoughts\n",
    "svm = SVC(C=0.1, gamma='scale', kernel='linear', class_weight = 'balanced')\n",
    "svm.fit(X_train, y_train[:,4])\n",
    "print(f'SVM classifier accuracy: {svm.score(X_train, y_train[:,4])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm.score(X_test, y_test[:,4])}')\n",
    "\n",
    "y_pred = svm.predict(X_train)\n",
    "print(f1_score(y_pred, y_train[:,4], average = \"macro\"))\n",
    "y_pred_test = svm.predict(X_test)\n",
    "print(f1_score(y_pred_test, y_test[:,4], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random over and undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC on the deataset with reduced dimensionality (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-normalized\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train_reduced)\n",
    "X_test_pca = pca.fit_transform(X_test_reduced)\n",
    "\n",
    "svm_pca = SVC(C=0.1, gamma='scale', kernel='linear', class_weight = 'balanced')\n",
    "svm_pca.fit(X_pca, y_train_reduced[:,0])\n",
    "print(f'SVM classifier accuracy: {svm_pca.score(X_pca, y_train_reduced[:,0])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm_pca.score(X_test_pca, y_test_reduced[:,0])}')\n",
    "y_pred = svm_pca.predict(X_pca)\n",
    "print(f1_score(y_pred, y_train_reduced[:,0], average = \"macro\"))\n",
    "y_pred_test = svm_pca.predict(X_test_pca)\n",
    "print(f1_score(y_pred_test, y_test_reduced[:,0], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train_reduced_norm)\n",
    "X_test_pca = pca.fit_transform(X_test_reduced_norm)\n",
    "\n",
    "svm_pca = SVC(C=0.1, gamma='scale', kernel='linear', class_weight = 'balanced')\n",
    "svm_pca.fit(X_pca, y_train_reduced_norm[:,0])\n",
    "print(f'SVM classifier accuracy: {svm_pca.score(X_pca, y_train_reduced_norm[:,0])}')\n",
    "print(f'SVM classifier accuracy - test set: {svm_pca.score(X_test_pca, y_test_reduced_norm[:,0])}')\n",
    "y_pred = svm_pca.predict(X_pca)\n",
    "print(f1_score(y_pred, y_train_reduced_norm[:,0], average = \"macro\"))\n",
    "y_pred_test = svm_pca.predict(X_test_pca)\n",
    "print(f1_score(y_pred_test, y_test_reduced_norm[:,0], average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_boundaries(X, y, model, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    colors = {0.0:'red', 1.0:'green'}  \n",
    "    print(Z.shape)\n",
    "    print(Z.ravel().shape)\n",
    "    print(X[:, 0].shape)\n",
    "    print(X[:, 1].shape)\n",
    "    \n",
    "    colored_labels = np.array([colors[xi] for xi in y]) #Z.ravel()\n",
    "    plt.contourf(xx, yy, Z, cmap='viridis')\n",
    "\n",
    "\n",
    "    cb = plt.colorbar()\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title(title)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colored_labels, cmap='viridis', s=10)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##can break the whole notebook on the non-normalized PCA\n",
    "plot_decision_boundaries(X_pca, y_train_reduced[:,0], svm_pca, 'Decision Boundaries')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "preds = svm_norm.predict(X_test_reduced_norm)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test_reduced[:,0], preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "preds = svm.predict(X_test_reduced)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test_reduced[:,0], preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
